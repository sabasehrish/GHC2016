@inproceedings{Lofstead:2013:IEI:2503210.2503238,
 author = {Lofstead, Jay and Ross, Robert},
 title = {Insights for Exascale IO APIs from Building a Petascale IO API},
 booktitle = {Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis},
 series = {SC '13},
 year = {2013},
 isbn = {978-1-4503-2378-9},
 location = {Denver, Colorado},
 pages = {87:1--87:12},
 articleno = {87},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/2503210.2503238},
 doi = {10.1145/2503210.2503238},
 acmid = {2503238},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@book{Stearley_Tomkins_VanDyke_Ferreira_Laros_Bridges_2013, 
 place={United States}, 
 title={Investigating an API for resilient exascale computing.}, 
 url={http://www.osti.gov/scitech/servlets/purl/1096503}, 
 DOI={10.2172/1096503}, 
 abstractNote={Increased HPC capability comes with increased complexity, part counts, and fault occurrences. In- creasing the resilience of systems and applications to faults is a critical requirement facing the viability of exascale systems, as the overhead of traditional checkpoint/restart is projected to outweigh its bene ts due to fault rates outpacing I/O bandwidths. As faults occur and propagate throughout hardware and software layers, pervasive noti cation and handling mechanisms are necessary. This report describes an initial investigation of fault types and programming interfaces to mitigate them. Proof-of-concept APIs are presented for the frequent and important cases of memory errors and node failures, and a strategy proposed for lesystem failures. These involve changes to the operating system, runtime, I/O library, and application layers. While a single API for fault handling among hardware and OS and application system-wide remains elusive, the e ort increased our understanding of both the mountainous challenges and the promising trailheads. 3}, 
 author={Stearley, Jon R. and Tomkins, James and VanDyke, John P. and Ferreira, Kurt Brian and Laros, James H. and Bridges, Patrick [University of New Mexico}, 
 year={2013}, 
 month={May}
}

@inbook{313da7c326a74c528baf67ea5473611c,
 title = "PXFS: A persistent storage model for extreme scale",
 keywords = "Computer Networks and Communications",
 publisher = "IEEE Computer Society",
 author = "Shuangyang Yang and Maciej Brodowicz and Ligon, {Walter B.} and Hartmut Kaiser",
 year = "2014",
 doi = "10.1109/ICCNC.2014.6785457",
 pages = "900-906",
 booktitle = "2014 International Conference on Computing, Networking and Communications, ICNC 2014",
}

@inproceedings{Li:2012:IOB:2357496.2358563,
 author = {Li, Dong and Vetter, Jeffrey S. and Marin, Gabriel and McCurdy, Collin and Cira, Cristian and Liu, Zhuo and Yu, Weikuan},
 title = {Identifying Opportunities for Byte-Addressable Non-Volatile Memory in Extreme-Scale Scientific Applications},
 booktitle = {Proceedings of the 2012 IEEE 26th International Parallel and Distributed Processing Symposium},
 series = {IPDPS '12},
 year = {2012},
 isbn = {978-0-7695-4675-9},
 pages = {945--956},
 numpages = {12},
 url = {http://dx.doi.org/10.1109/IPDPS.2012.89},
 doi = {10.1109/IPDPS.2012.89},
 acmid = {2358563},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@inproceedings{GamellRPP13,
  author    = {Marc Gamell and
               Ivan Rodero and
               Manish Parashar and
               Stephen W. Poole},
  title     = {Exploring energy and performance behaviors of data-intensive scientific
               workflows on systems with deep memory hierarchies},
  booktitle = {20th Annual International Conference on High Performance Computing,
               HiPC 2013, Bengaluru (Bangalore), Karnataka, India, December 18-21,
               2013},
  pages     = {226--235},
  year      = {2013},
  url       = {http://dx.doi.org/10.1109/HiPC.2013.6799122},
  doi       = {10.1109/HiPC.2013.6799122},
  timestamp = {Thu, 18 Sep 2014 16:58:23 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/hipc/GamellRPP13},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@inproceedings{JinZSBPKKCHCP14,
  author    = {Tong Jin and
               Fan Zhang and
               Qian Sun and
               Hoang Bui and
               Norbert Podhorszki and
               Scott Klasky and
               Hemanth Kolla and
               Jacqueline Chen and
               Robert Hager and
               Choong{-}Seock Chang and
               Manish Parashar},
  title     = {Leveraging deep memory hierarchies for data staging in coupled data-intensive
               simulation workflows},
  booktitle = {2014 {IEEE} International Conference on Cluster Computing, {CLUSTER}
               2014, Madrid, Spain, September 22-26, 2014},
  pages     = {268--269},
  year      = {2014},
  url       = {http://dx.doi.org/10.1109/CLUSTER.2014.6968744},
  doi       = {10.1109/CLUSTER.2014.6968744},
  timestamp = {Tue, 02 Dec 2014 17:10:04 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/cluster/JinZSBPKKCHCP14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@inproceedings{Kannan:2011:UAN:2110205.2110209,
 author = {Kannan, Sudarsun and Gavrilovska, Ada and Schwan, Karsten and Milojicic, Dejan and Talwar, Vanish},
 title = {Using Active NVRAM for I/O Staging},
 booktitle = {Proceedings of the 2Nd International Workshop on Petascal Data Analytics: Challenges and Opportunities},
 series = {PDAC '11},
 year = {2011},
 isbn = {978-1-4503-1130-4},
 location = {Seattle, Washington, USA},
 pages = {15--22},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/2110205.2110209},
 doi = {10.1145/2110205.2110209},
 acmid = {2110209},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DataStaging, non-volatile memory, post processing},
} 

@inproceedings{SayedGHPSSS13,
  added-at = {2013-06-10T00:00:00.000+0200},
  author = {Sayed, Salem El and Graf, Stephan and Hennecke, Michael and Pleiter, Dirk and Schwarz, Georg and Schick, Heiko and Stephan, Michael},
  biburl = {http://www.bibsonomy.org/bibtex/29bd4bab5f98f7010a5d63235200a57c7/dblp},
  booktitle = {ISC},
  editor = {Kunkel, Julian M. and 0002, Thomas Ludwig and Meuer, Hans Werner},
  ee = {http://dx.doi.org/10.1007/978-3-642-38750-0_33},
  interhash = {ad7f0f490ea305a77687f710b91d9e1e},
  intrahash = {9bd4bab5f98f7010a5d63235200a57c7},
  isbn = {978-3-642-38749-4},
  keywords = {dblp},
  pages = {435-446},
  publisher = {Springer},
  series = {Lecture Notes in Computer Science},
  timestamp = {2013-08-13T15:20:37.000+0200},
  title = {Using GPFS to Manage NVRAM-Based Storage Cache.},
  url = {http://dblp.uni-trier.de/db/conf/supercomputer/isc2013.html#SayedGHPSSS13},
  volume = 7905,
  year = 2013
}

@article{mittal2015survey,
   title={A Survey of Software Techniques for Using Non-Volatile Memories for Storage and Main Memory Systems},
   year={2015},
   author={Sparsh Mittal and Jeffrey S Vetter},
   journal={IEEE Transactions on Parallel and Distributed Systems (TPDS) (impact factor 2.17)},
   url={https://www.academia.edu/12769873/A_Survey_of_Software_Techniques_for_Using_Non-Volatile_Memories_for_Storage_and_Main_Memory_Systems},
   keywords={non-volatile memory, NVM, NVRAM, flash memory, phase change RAM, PCM, spin transfer torque RAM, STT-RAM, STT-MRAM, resistive RAM, ReRAM, RRAM, storage class memory, SSD}
}

@INPROCEEDINGS{Liu12onthe,
    author = {Ning Liu and Jason Cope and Philip Carns and Christopher Carothers and Robert Ross and Gary Grider and Adam Crume and Carlos Maltzahn},
    title = {On the role of burst buffers in leadership-class storage systems},
    booktitle = {In Proceedings of the 2012 IEEE Conference on Massive Data Storage},
    year = {2012}
}

@article{Pelley:2013:SMN:2732228.2732231,
 author = {Pelley, Steven and Wenisch, Thomas F. and Gold, Brian T. and Bridge, Bill},
 title = {Storage Management in the NVRAM Era},
 journal = {Proc. VLDB Endow.},
 issue_date = {October 2013},
 volume = {7},
 number = {2},
 month = oct,
 year = {2013},
 issn = {2150-8097},
 pages = {121--132},
 numpages = {12},
 url = {http://dx.doi.org/10.14778/2732228.2732231},
 doi = {10.14778/2732228.2732231},
 acmid = {2732231},
 publisher = {VLDB Endowment},
} 

@inproceedings{kannan2011using,
  title={Using active NVRAM for cloud I/O},
  author={Kannan, Sudarsun and Milojicic, Dejan and Talwar, Vanish and Gavrilovska, Ada and Schwan, Karsten and Abbasi, Hasan},
  booktitle={Open Cirrus Summit (OCS), 2011 Sixth},
  pages={32--36},
  year={2011},
  organization={IEEE}
}
@article{multicms,
  author={E Sexton-Kennedy and Patrick Gartung and C D Jones and David Lange},
  title={Implementation of a Multi-threaded Framework for Large- scale Scientific Applications},
  journal={Journal of Physics: Conference Series},
  volume={608},
  number={1},
  pages={012034},
  url={http://stacks.iop.org/1742-6596/608/i=1/a=012034},
  year={2015},
  abstract={The CMS experiment has recently completed the development of a multi-threaded capable application framework. In this paper, we will discuss the design, implementation and application of this framework to production applications in CMS. For the 2015 LHC run, this functionality is particularly critical for both our online and offline production applications, which depend on faster turn-around times and a reduced memory footprint relative to before. These applications are complex codes, each including a large number of physics-driven algorithms. While the framework is capable of running a mix of thread-safe and "legacy" modules, algorithms running in our production applications need to be thread-safe for optimal use of this multi-threaded framework at a large scale. Towards this end, we discuss the types of changes, which were necessary for our algorithms to achieve good performance of our multithreaded applications in a full-scale application. Finally performance numbers for what has been achieved for the 2015 run are presented.}
}

@inproceedings{xin2013graphx,
  title={Graphx: A resilient distributed graph system on spark},
  author={Xin, Reynold S and Gonzalez, Joseph E and Franklin, Michael J and Stoica, Ion},
  booktitle={First International Workshop on Graph Data Management Experiences and Systems},
  pages={2},
  year={2013},
  organization={ACM}
}

@Misc{graphX,
   title = {GraphX},
   howpublished = {\url{https://spark.apache.org/docs/latest/graphx-programming-guide.html}},
   }

@inproceedings{Zaharia:2010:SCC:1863103.1863113,
 author = {Zaharia, Matei and Chowdhury, Mosharaf and Franklin, Michael J. and Shenker, Scott and Stoica, Ion},
 title = {Spark: Cluster Computing with Working Sets},
 booktitle = {Proceedings of the 2Nd USENIX Conference on Hot Topics in Cloud Computing},
 series = {HotCloud'10},
 year = {2010},
 location = {Boston, MA},
 pages = {10--10},
 numpages = {1},
 url = {http://dl.acm.org/citation.cfm?id=1863103.1863113},
 acmid = {1863113},
 publisher = {USENIX Association},
 address = {Berkeley, CA, USA},
} 

@Misc{spark,
   title = {Spark.},
   howpublished = {\url{https://spark.apache.org}},
   }

@Misc{dataframe,
   title = {{DataFrame API.}},
   key = {{DataFrame API.}},
   howpublished = {\url{https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html}},
   }

@incollection{liu2012modeling,
  title={Modeling a leadership-scale storage system},
  author={Liu, Ning and Carothers, Christopher and Cope, Jason and Carns, Philip and Ross, Robert and Crume, Adam and Maltzahn, Carlos},
  booktitle={Parallel Processing and Applied Mathematics},
  pages={10--19},
  year={2012},
  publisher={Springer}
}

@article{hacc, 
title = {{HACC: Simulating Sky Surveys on State-of-the-Art Supercomputing Architectures}},
key = {HACC: Simulating Sky Surveys on State-of-the-Art Supercomputing Architectures},
author = "Salman Habib and Adrian Pope and Hal Finkel and Nicholas Frontiere and Katrin Heitmann and David Daniel and Patricia Fasel and Vitali Morozov and George Zagaris and Tom Peterka and Venkatram Vishwanath and Zarija Lukic and Saba Sehrish and Wei-keng Liao", 
doi = "arXiv:1410.2805"
}

@Misc{damsel,
 key = {DAMSEL - A Data Model Storage Library for Exascale Science}, 
 title = {{DAMSEL - A Data Model Storage Library for Exascale Science}}, 
 howpublished = {\url{http://cucis.ece.northwestern.edu/projects/DAMSEL}},
}

@article{art_1,
  author={C Green and J Kowalkowski and M Paterno and M Fischler and L Garren and Q Lu},
  title={The art framework},
  journal={Journal of Physics: Conference Series},
  volume={396},
  number={2},
  pages={022020},
  url={http://stacks.iop.org/1742-6596/396/i=2/a=022020},
  year={2012},
  abstract={Future “Intensity Frontier” experiments at Fermilab are likely to be conducted by smaller collaborations, with fewer scientists, than is the case for recent “Energy Frontier” experiments. art is a C++ event-processing framework designed with the needs of such experiments in mind. An evolution from the framework of the CMS experiment, art was designed and implemented to be usable by multiple experiments without imposing undue maintenance effort requirements on either the art developers or experiments using it. We describe the key requirements and features of art and the rationale behind evolutionary changes, additions and simplifications with respect to the CMS framework. In addition, our package distribution system and our collaborative model with respect to the multiple experiments using art helps keep the maintenance burden low. We also describe in-progress and future enhancements to the framework, including strategies we are using to allow multi-threaded use of the art framework in today's multi- and many-core environments.}
}

@article{art_2,
  author={Tasha Arvanitis and Adam Lyon},
  title={artG4: A Generic Framework for Geant4 Simulations},
  journal={Journal of Physics: Conference Series},
  volume={513},
  number={2},
  pages={022023},
  url={http://stacks.iop.org/1742-6596/513/i=2/a=022023},
  year={2014},
  abstract={A small experiment must devote its limited computing expertise to writing physics code directly applicable to the experiment. A software "framework" is essential for providing an infrastructure that makes writing the physics-relevant code easy. In this paper, we describe a highly modular and easy to use framework for writing Geant4 based simulations called "artg4". This framework is a layer on top of the art framework.}
}

@article{cms,
  author={The CMS Collaboration and S Chatrchyan et. al.},
  title={The CMS experiment at the CERN LHC},
  journal={Journal of Instrumentation},
  volume={3},
  number={08},
  pages={S08004},
  url={http://stacks.iop.org/1748-0221/3/i=08/a=S08004},
  year={2008},
  abstract={The Compact Muon Solenoid (CMS) detector is described. The detector operates at the Large Hadron Collider (LHC) at CERN. It was conceived to study proton-proton (and lead-lead) collisions at a centre-of-mass energy of 14 TeV (5.5 TeV nucleon-nucleon) and at luminosities up to 10 34 cm −2 s −1 (10 27 cm −2 s −1 ). At the core of the CMS detector sits a high-magnetic-field and large-bore superconducting solenoid surrounding an all-silicon pixel and strip tracker, a lead-tungstate scintillating-crystals electromagnetic calorimeter, and a brass-scintillator sampling hadron calorimeter. The iron yoke of the flux-return is instrumented with four stations of muon detectors covering most of the 4π solid angle. Forward sampling calorimeters extend the pseudorapidity coverage to high values (|η| ≤ 5) assuring very good hermeticity. The overall dimensions of the CMS detector are a length of 21.6 m, a diameter of 14.6 m and a total weight of 12500 t.}
}

@article{cmss,
  author={P Elmer and B Hegner and L Sexton-Kennedy},
  title={Experience with the CMS event data model},
  journal={Journal of Physics: Conference Series},
  volume={219},
  number={3},
  pages={032022},
  url={http://stacks.iop.org/1742-6596/219/i=3/a=032022},
  year={2010},
  abstract={The re-engineered CMS EDM was presented at CHEP in 2006. Since that time we have gained a lot of operational experience with the chosen model. We will present some of our findings, and attempt to evaluate how well it is meeting its goals. We will discuss some of the new features that have been added since 2006 as well as some of the problems that have been addressed. Also discussed is the level of adoption throughout CMS, which spans the trigger farm up to the final physics analysis. Future plans, in particular dealing with schema evolution and scaling, will be discussed briefly.}
}

@article{cmstracking,
  author={The CMS Collaboration},
  title={Description and performance of track and primary-vertex reconstruction with the CMS tracker},
  journal={Journal of Instrumentation},
  volume={9},
  number={10},
  pages={P10009},
  url={http://stacks.iop.org/1748-0221/9/i=10/a=P10009},
  year={2014},
  abstract={A description is provided of the software algorithms developed for the CMS tracker both for reconstructing charged-particle trajectories in proton-proton interactions and for using the resulting tracks to estimate the positions of the LHC luminous region and individual primary-interaction vertices. Despite the very hostile environment at the LHC, the performance obtained with these algorithms is found to be excellent. For t ##IMG## [http://ej.iop.org/icons/Entities/bart.gif] {bar t} events under typical 2011 pileup conditions, the average track-reconstruction efficiency for promptly-produced charged particles with transverse momenta of p T > 0.9GeV is 94% for pseudorapidities of |η| < 0.9 and 85% for 0.9 < |η| < 2.5. The inefficiency is caused mainly by hadrons that undergo nuclear interactions in the tracker material. For isolated muons, the corresponding efficiencies are essentially 100%. For isolated muons of p T  = 100GeV emitted at |η| < 1.4, the resolutions are approximately 2.8% in p T , and respectively, 10μm and 30μm in the transverse and longitudinal impact parameters. The position resolution achieved for reconstructed primary vertices that correspond to interesting pp collisions is 10–12μm in each of the three spatial dimensions. The tracking and vertexing software is fast and flexible, and easily adaptable to other functions, such as fast tracking for the trigger, or dedicated tracking for electrons that takes into account bremsstrahlung.}
}

@article{cmsdata,
  author={Jennifer Adelman-McCarthy et. al.},
  title={CMS distributed computing workflow experience},
  journal={Journal of Physics: Conference Series},
  volume={331},
  number={7},
  pages={072019},
  url={http://stacks.iop.org/1742-6596/331/i=7/a=072019},
  year={2011},
  abstract={The vast majority of the CMS Computing capacity, which is organized in a tiered hierarchy, is located away from CERN. The 7 Tier-1 sites archive the LHC proton-proton collision data that is initially processed at CERN. These sites provide access to all recorded and simulated data for the Tier-2 sites, via wide-area network (WAN) transfers. All central data processing workflows are executed at the Tier-1 level, which contain re-reconstruction and skimming workflows of collision data as well as reprocessing of simulated data to adapt to changing detector conditions. This paper describes the operation of the CMS processing infrastructure at the Tier-1 level. The Tier-1 workflows are described in detail. The operational optimization of resource usage is described. In particular, the variation of different workflows during the data taking period of 2010, their efficiencies and latencies as well as their impact on the delivery of physics results is discussed and lessons are drawn from this experience. The simulation of proton-proton collisions for the CMS experiment is primarily carried out at the second tier of the CMS computing infrastructure. Half of the Tier-2 sites of CMS are reserved for central Monte Carlo (MC) production while the other half is available for user analysis. This paper summarizes the large throughput of the MC production operation during the data taking period of 2010 and discusses the latencies and efficiencies of the various types of MC production workflows. We present the operational procedures to optimize the usage of available resources and we the operational model of CMS for including opportunistic resources, such as the larger Tier-3 sites, into the central production operation.},
}

@Misc{olis-exascale-whitepaper,
   author = {Bloom K. and Gutsche O.},
   title = {{DOE} {SC} Exascale Requirements Reviews: High Energy Physics: Non-Traditional {HPC} Use Case: Energy Frontier Experiment},
   month = {June},
   year = {2015},
   howpublished = {\url{https://asset1.basecamp.com/2126401/projects/9226863/attachments/161144144}},
   }

@Misc{uboone,
   author = {The {MicroBooNE} Collaboration},
   title = {The {MicroBooNE} Technical Design Report},
   month = {February},
   year = {2012},
   howpublished = {\url{http://www-microboone.fnal.gov/publications/TDRCD3.pdf}},
   }
